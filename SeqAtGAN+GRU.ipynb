{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwxgp6mYYUF3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import torch\n",
        "\n",
        "class cfg:\n",
        "    FOLD = 0\n",
        "    SEEDS = [42]\n",
        "    batch_size = 64\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data = pd.read_csv('dynamic_api_call_sequence_per_malware_100_0_306.csv')\n",
        "\n",
        "data1 = data.drop(columns=['hash'],\n",
        "\n",
        "                 axis=1)\n",
        "data1 = data1.dropna(how='any')\n",
        "print(data1.shape)\n",
        "data_dict = {}\n",
        "for label, group in data1.groupby('malware'):\n",
        "    data_dict[label] = group.sample(min(group.shape[0], 4000))\n",
        "data2 = pd.concat(data_dict.values())\n",
        "data2 = data2.reset_index()\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "for fold, (_, val_idx) in enumerate(skf.split(data2, data2.malware)):\n",
        "    data2.loc[val_idx, 'fold'] = fold\n",
        "data2.fold = data2.fold.astype(int)\n",
        "data2.to_csv('malware-analysis-1000.csv')\n",
        "X_train = data2.query(f'fold!={cfg.FOLD}').drop(columns=['index', 'malware', 'fold'])\n",
        "X_test = data2.query(f'fold=={cfg.FOLD}').drop(columns=['index', 'malware', 'fold'])\n",
        "\n",
        "Y_train = data2.query(f'fold!={cfg.FOLD}').malware\n",
        "Y1_test = data2.query(f'fold=={cfg.FOLD}').malware\n",
        "\n",
        "# Checking the sizes of the datasets\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_res, y1_train_res = smote.fit_resample(X_train, Y_train)\n",
        "\n",
        "# Check the distribution of classes after resampling\n",
        "print(pd.Series(y1_train_res).value_counts())\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data, transform the test data\n",
        "X1_train_res = scaler.fit_transform(X_train_res)\n",
        "X1_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biRHvciTaPVI"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('MalBehavD-V1-dataset.csv')\n",
        "\n",
        "# Tokenization - Break API call strings into tokens\n",
        "df['api_calls'] = df.iloc[:, 2:].apply(lambda row: ' '.join(row.astype(str)), axis=1)\n",
        "\n",
        "# Tokenizing API call sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['api_calls'])\n",
        "X_tokenized = tokenizer.texts_to_sequences(df['api_calls'])\n",
        "\n",
        "# Padding Sequences to ensure uniformity of input length\n",
        "max_len = min(max(df['api_calls'].apply(lambda x: len(x.split()))), 100)\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# One-Hot Encoding the 'labels' column\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_one_hot = encoder.fit_transform(df[['labels']])\n",
        "\n",
        "# Min-Max Normalization for numerical columns\n",
        "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "df[numeric_columns] = MinMaxScaler().fit_transform(df[numeric_columns])\n",
        "print(\"Preprocessing completed successfully!\")\n",
        "\n",
        "# Prepare data for splitting into train/test sets\n",
        "X = df.drop(columns=['labels'])\n",
        "y = df['labels']\n",
        "\n",
        "# Split into 70% training and 30% testing\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "# Check the size of the splits\n",
        "print(f\"Training set size: {X2_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X2_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTZ5pOMValQY"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "csv_path = \"malware_API_dataset.csv\"\n",
        "data = []\n",
        "\n",
        "with open(csv_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        # row[0] = malware_class\n",
        "        # row[1] = sha256 (hash)\n",
        "        # row[2:] = actual API call strings, e.g. [\"NtOpenFile\", \"ReadFile\", \"CloseHandle\"]\n",
        "        malware_class = row[0]\n",
        "        sha256        = row[1]\n",
        "        api_calls     = row[2:]\n",
        "\n",
        "        # Convert \"malware_class\" to 0 or 1\n",
        "        # (If it contains \"not-a-virus\" => 0, else 1)\n",
        "        label = 0 if \"not-a-virus\" in malware_class else 1\n",
        "\n",
        "        # Join all API calls into a single space-delimited string\n",
        "        api_calls_str = \" \".join(api_calls)\n",
        "\n",
        "        data.append([label, sha256, api_calls_str])\n",
        "\n",
        "# Create a DataFrame\n",
        "df_processed = pd.DataFrame(data, columns=[\"malware_class\", \"sha256\", \"api_calls\"])\n",
        "\n",
        "# Drop 'sha256'\n",
        "df_processed.drop(columns=[\"sha256\"], inplace=True)\n",
        "print(\"Sample rows:\\n\", df_processed.head())\n",
        "\n",
        "# MinMax Scale any numeric cols\n",
        "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "numeric_cols = [col for col in numeric_cols if col != \"malware_class\"]\n",
        "\n",
        "if len(numeric_cols) > 0:\n",
        "    scaler = MinMaxScaler()\n",
        "    df_processed[numeric_cols] = scaler.fit_transform(df_processed[numeric_cols])\n",
        "\n",
        "# Tokenization & Padding for the 'api_calls' col\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_processed[\"api_calls\"])\n",
        "X3_sequences = tokenizer.texts_to_sequences(df_processed[\"api_calls\"])\n",
        "\n",
        "# Pad the sequences\n",
        "MAX_LEN = 300\n",
        "X3_padded = pad_sequences(X3_sequences, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "# One-Hot Encode the Label\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y3_onehot = encoder.fit_transform(df_processed[[\"malware_class\"]])\n",
        "\n",
        "print(\"X3_padded shape =\", X3_padded.shape)\n",
        "print(\"y3_onehot shape =\", y3_onehot.shape)\n",
        "y3 = df_processed[\"malware_class\"].values\n",
        "print(\"y3 shape =\", y3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36cQbdNja2qT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Combine API Sequences from Dataset 1 & 2\n",
        "MAX_LEN = 300\n",
        "\n",
        "# Combine API calls and encode malware labels\n",
        "data2['api_calls'] = data2.drop(columns=['index', 'malware', 'fold']).astype(str).agg(' '.join, axis=1)\n",
        "data2['malware_label'] = data2['malware'].astype('category').cat.codes\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data2['api_calls'].tolist() + df['api_calls'].tolist())\n",
        "\n",
        "X1 = pad_sequences(tokenizer.texts_to_sequences(data2['api_calls']), maxlen=MAX_LEN, padding='post')\n",
        "y1 = data2['malware_label'].values\n",
        "\n",
        "X2 = pad_sequences(tokenizer.texts_to_sequences(df['api_calls']), maxlen=MAX_LEN, padding='post')\n",
        "y2 = df['labels'].values\n",
        "\n",
        "# Combine datasets\n",
        "X_combined = np.vstack([X1, X2])\n",
        "y_combined = np.hstack([y1, y2])\n",
        "\n",
        "# Split into training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.3, random_state=42)\n",
        "\n",
        "# PyTorch Dataset & Dataloader\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.float).unsqueeze(1)  # Ensure the shape is (batch_size, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#import torch.utils.checkpoint as checkpoint\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import math\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LEN = 300\n",
        "batch_size = 64\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "learning_rate = 0.0004\n",
        "epochs = 20\n",
        "accumulation_steps = 4\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Gumbel-Softmax\n",
        "def sample_gumbel_softmax(logits, tau=1.0):\n",
        "    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-9) + 1e-9)\n",
        "    y = logits + gumbel_noise\n",
        "    return torch.nn.functional.softmax(y / tau, dim=-1)\n",
        "\n",
        "# Generator with Dropout Regularization\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, dropout=0.3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        output = self.fc(self.dropout(output))\n",
        "        return output\n",
        "\n",
        "\n",
        "# Discriminator with GRU and Multihead Attention\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, dropout=0.3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim * 2, num_heads=4, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        rnn_output, _ = self.rnn(x)\n",
        "        attn_output, _ = self.attn(rnn_output, rnn_output, rnn_output)\n",
        "        output = self.fc(self.dropout(attn_output[:, -1, :]))\n",
        "        return output\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.01)\n",
        "    elif isinstance(m, nn.GRU):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param.data)\n",
        "            elif 'bias' in name:\n",
        "                param.data.fill_(0.01)\n",
        "\n",
        "# Gradient Penalty Function (WGAN-GP style)\n",
        "def gradient_penalty(discriminator, real_data, fake_data):\n",
        "    batch_size = real_data.size(0)\n",
        "    epsilon = torch.rand(batch_size, 1, 1).to(device)\n",
        "    interpolated = (epsilon * real_data.unsqueeze(2) + (1 - epsilon) * fake_data.unsqueeze(2)).type(torch.float).requires_grad_(True).to(device)\n",
        "    d_interpolated = discriminator(interpolated.long().squeeze(2))\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolated,\n",
        "        inputs=interpolated,\n",
        "        grad_outputs=torch.ones_like(d_interpolated).to(device),\n",
        "        allow_unused=True,\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]\n",
        "\n",
        "    if gradients is not None:\n",
        "        gradients = gradients.view(batch_size, -1)\n",
        "    else:\n",
        "        gradients = torch.zeros_like(interpolated).to(device)\n",
        "    gradient_norm = gradients.norm(2, dim=1)\n",
        "    return ((gradient_norm - 1) ** 2).mean()\n",
        "\n",
        "# Feature Matching Loss\n",
        "def feature_matching_loss(real_features, fake_features):\n",
        "    return torch.mean((real_features - fake_features) ** 2)\n",
        "\n",
        "# Dataset Class\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "#Pretrain Generator\n",
        "def pretrain_generator(generator, train_loader, gen_optimizer, vocab_size, device='cuda', epochs=5):\n",
        "    print(\"📘 Starting Generator Pretraining...\")\n",
        "    generator.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for sequences, _ in train_loader:\n",
        "            sequences = sequences.to(device)\n",
        "            inputs = sequences[:, :-1]  # All tokens except the last\n",
        "            targets = sequences[:, 1:]  # All tokens except the first (shifted)\n",
        "\n",
        "            gen_optimizer.zero_grad()\n",
        "            logits = generator(inputs)\n",
        "            logits = logits.view(-1, vocab_size)\n",
        "            targets = targets.reshape(-1)\n",
        "\n",
        "            loss = criterion(logits, targets)\n",
        "            loss.backward()\n",
        "            gen_optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Pretrain Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "# Training Function (with Gradient Penalty and Feature Matching)\n",
        "def train_adversarial(generator, discriminator, gen_optimizer, disc_optimizer, criterion, train_loader, val_loader, device, tau, vocab_size, lambda_gp=10, lambda_feat=5):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    for i, (real_sequences, real_labels) in enumerate(train_loader):\n",
        "        real_sequences, real_labels = real_sequences.to(device), real_labels.to(device)\n",
        "\n",
        "        # Discriminator training\n",
        "        disc_optimizer.zero_grad()\n",
        "        real_output = discriminator(real_sequences)\n",
        "        real_loss = criterion(real_output, real_labels)\n",
        "\n",
        "        noise = torch.randint(0, vocab_size, (real_sequences.size(0), MAX_LEN)).to(device)\n",
        "        fake_logits = generator(noise)\n",
        "        fake_probs = sample_gumbel_softmax(fake_logits, tau=tau)\n",
        "        fake_indices = torch.multinomial(fake_probs.view(-1, vocab_size), 1).view(real_sequences.size(0), MAX_LEN)\n",
        "        fake_output = discriminator(fake_indices.detach())\n",
        "        fake_loss = criterion(fake_output, torch.zeros_like(real_labels))\n",
        "\n",
        "        # Calculate Gradient Penalty\n",
        "        gp_loss = gradient_penalty(discriminator, real_sequences, fake_indices)\n",
        "\n",
        "        disc_loss = real_loss + fake_loss + lambda_gp * gp_loss\n",
        "        disc_loss.backward()\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            disc_optimizer.step()\n",
        "            disc_optimizer.zero_grad()\n",
        "\n",
        "        # Generator training\n",
        "        gen_optimizer.zero_grad()\n",
        "        fake_logits = generator(noise)\n",
        "        fake_probs = sample_gumbel_softmax(fake_logits, tau=tau)\n",
        "        fake_indices = torch.argmax(fake_probs, dim=-1)\n",
        "        fake_output_gen = discriminator(fake_indices)\n",
        "        gen_loss = criterion(fake_output_gen, torch.ones_like(real_labels))\n",
        "\n",
        "        # Feature Matching Loss\n",
        "        real_features = discriminator(real_sequences)\n",
        "        fake_features = discriminator(fake_indices)\n",
        "        feat_loss = feature_matching_loss(real_features, fake_features)\n",
        "\n",
        "        gen_loss += lambda_feat * feat_loss\n",
        "        gen_loss.backward()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            gen_optimizer.step()\n",
        "            gen_optimizer.zero_grad()\n",
        "\n",
        "        total_train_loss += disc_loss.item()\n",
        "\n",
        "    # Validation Loss\n",
        "    discriminator.eval()\n",
        "    with torch.no_grad():\n",
        "        for real_sequences, real_labels in val_loader:\n",
        "            real_sequences, real_labels = real_sequences.to(device), real_labels.to(device)\n",
        "            val_output = discriminator(real_sequences)\n",
        "            val_loss = criterion(val_output, real_labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    return total_train_loss / len(train_loader), total_val_loss / len(val_loader)\n",
        "\n",
        "\n",
        "# Evaluation Function with Threshold Adjustment\n",
        "def evaluate_model(discriminator, test_loader, device, threshold=0.4):\n",
        "    discriminator.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in test_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            preds = discriminator(sequences)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred.round(), average='weighted', zero_division=0)\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred.round())\n",
        "    return precision, recall, f1, auc, accuracy\n",
        "\n",
        "# =======================================\n",
        "# 🧪 Dataset Preparation\n",
        "# Normalize binary labels\n",
        "data2['malware_label'] = data2['malware'].astype('category').cat.codes\n",
        "data2['malware_label'] = data2['malware_label'].apply(lambda x: 0 if x == 0 else 1)\n",
        "df['labels'] = df['labels'].apply(lambda x: 0 if x == 0 else 1)\n",
        "\n",
        "# Build API call sequences\n",
        "data2['api_calls'] = data2.drop(columns=['index', 'malware', 'fold']).astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data2['api_calls'].tolist() + df['api_calls'].tolist())\n",
        "X1 = pad_sequences(tokenizer.texts_to_sequences(data2['api_calls']), maxlen=MAX_LEN, padding='post')\n",
        "X2 = pad_sequences(tokenizer.texts_to_sequences(df['api_calls']), maxlen=MAX_LEN, padding='post')\n",
        "y1 = data2['malware_label'].values\n",
        "y2 = df['labels'].values\n",
        "df_processed = df_processed[df_processed[\"malware_class\"].notna()]\n",
        "df_processed['api_calls'] = df_processed.iloc[:, 1:].apply(lambda row: ' '.join(row.astype(str)), axis=1)\n",
        "X3_tokenized = tokenizer.texts_to_sequences(df_processed['api_calls'])\n",
        "X3_padded = pad_sequences(X3_tokenized, maxlen=MAX_LEN, padding='post')\n",
        "y3 = df_processed['malware_class'].values\n",
        "\n",
        "X3_train, X3_test, y3_train, y3_test = train_test_split(X3_padded, y3, test_size=0.8, random_state=42, stratify=y3)\n",
        "X_combined = np.vstack([X1, X2, X3_train])\n",
        "y_combined = np.hstack([y1, y2, y3_train])\n",
        "\n",
        "# Split data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_combined, y_combined, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(SequenceDataset(X_val, y_val), batch_size=batch_size)\n",
        "test_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "# Model Init\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "generator = Generator(vocab_size, embedding_dim, hidden_dim, vocab_size).to(device)\n",
        "discriminator = Discriminator(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "generator.apply(init_weights)\n",
        "discriminator.apply(init_weights)\n",
        "gen_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "disc_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "scheduler_gen = torch.optim.lr_scheduler.StepLR(gen_optimizer, step_size=5, gamma=0.5)\n",
        "scheduler_disc = torch.optim.lr_scheduler.StepLR(disc_optimizer, step_size=5, gamma=0.5)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Pretraining\n",
        "print(\"🔧 Pretraining Generator...\")\n",
        "pretrain_generator(generator, train_loader, gen_optimizer, vocab_size)\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Cross-validation settings\n",
        "num_folds = 5\n",
        "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Store metrics\n",
        "fold_metrics = []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X_combined, y_combined)):\n",
        "    print(f\"\\n================ Fold {fold + 1} / {num_folds} ================\")\n",
        "\n",
        "    X_train, X_test = X_combined[train_idx], X_combined[test_idx]\n",
        "    y_train, y_test = y_combined[train_idx], y_combined[test_idx]\n",
        "\n",
        "    # Split validation from train\n",
        "    X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=fold)\n",
        "\n",
        "    # Dataloaders for this fold\n",
        "    train_loader = DataLoader(SequenceDataset(X_train_split, y_train_split), batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(SequenceDataset(X_val, y_val), batch_size=batch_size)\n",
        "    test_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "    # Initialize models and optimizers fresh for each fold\n",
        "    generator = Generator(vocab_size, embedding_dim, hidden_dim, vocab_size).to(device)\n",
        "    discriminator = Discriminator(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "    gen_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "    disc_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    # Adversarial Training per fold\n",
        "    for epoch in range(epochs):\n",
        "        tau = max(0.5, 1.5 * math.exp(-0.1 * epoch))\n",
        "        train_loss, val_loss = train_adversarial(generator, discriminator, gen_optimizer, disc_optimizer, criterion, train_loader, val_loader, device, tau, vocab_size)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        scheduler_gen.step()\n",
        "        scheduler_disc.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, tau={tau:.4f}, Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # Final Evaluation on test set of this fold\n",
        "    precision, recall, f1, auc, accuracy = evaluate_model(discriminator, test_loader, device, threshold=0.4)\n",
        "    print(f\"\\n📊 Fold {fold+1} Evaluation: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}, AUC={auc:.4f}, Accuracy={accuracy:.4f}\")\n",
        "    plt.plot(range(epochs), train_losses, label='Training Loss')\n",
        "    plt.plot(range(epochs), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # Save metrics for averaging later\n",
        "    fold_metrics.append({\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "        'accuracy': accuracy\n",
        "    })\n",
        "\n",
        "# ====================\n",
        "# 📈 Overall Metrics\n",
        "# ====================\n",
        "\n",
        "print(\"\\n================ Cross-Validation Summary ================\")\n",
        "all_precisions = [m['precision'] for m in fold_metrics]\n",
        "all_recalls = [m['recall'] for m in fold_metrics]\n",
        "all_f1s = [m['f1'] for m in fold_metrics]\n",
        "all_aucs = [m['auc'] for m in fold_metrics]\n",
        "all_accuracies = [m['accuracy'] for m in fold_metrics]\n",
        "\n",
        "print(f\"Avg Precision: {np.mean(all_precisions):.4f}\")\n",
        "print(f\"Avg Recall:    {np.mean(all_recalls):.4f}\")\n",
        "print(f\"Avg F1 Score:  {np.mean(all_f1s):.4f}\")\n",
        "print(f\"Avg AUC:       {np.mean(all_aucs):.4f}\")\n",
        "print(f\"Avg Accuracy:  {np.mean(all_accuracies):.4f}\")\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.show()\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in test_loader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        preds = discriminator(sequences)\n",
        "        probs = torch.sigmoid(preds)  # Convert logits to probabilities\n",
        "        pred_labels = (probs > 0.4).float()  # Adjust threshold to improve recall\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(pred_labels.cpu().numpy())\n",
        "\n",
        "plot_confusion_matrix(np.array(y_true), np.array(y_pred))"
      ],
      "metadata": {
        "id": "s2iD0SHgfrKP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "aa82f8c9-09a0-464d-ee99-1eed6eca37de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Pretraining Generator...\n",
            "📘 Starting Generator Pretraining...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0a7f8bcffc5a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;31m# Pretraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🔧 Pretraining Generator...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m \u001b[0mpretrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0a7f8bcffc5a>\u001b[0m in \u001b[0;36mpretrain_generator\u001b[0;34m(generator, train_loader, gen_optimizer, vocab_size, device, epochs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# All tokens except the last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# All tokens except the first (shifted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m             )\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cuda_getDeviceCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             raise AssertionError(\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 3 DataLoader for testing\n",
        "X3_tensor = torch.tensor(X3_test, dtype=torch.long)\n",
        "y3_tensor = torch.tensor(y3_test, dtype=torch.float)\n",
        "dataset3_loader = DataLoader(SequenceDataset(X3_tensor, y3_tensor), batch_size=batch_size)\n",
        "\n",
        "# Final Evaluation on Dataset 3 (unseen test set)\n",
        "precision3, recall3, f1_3, auc3, accuracy = evaluate_model(discriminator, dataset3_loader, device)\n",
        "print(f\"Evaluation Metrics on Dataset 3 - Precision: {precision3:.4f}, Recall: {recall3:.4f}, F1: {f1_3:.4f}, AUC: {auc3:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in test_loader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        preds = discriminator(sequences)\n",
        "        probs = torch.sigmoid(preds)  # Convert logits to probabilities\n",
        "        pred_labels = (probs > 0.4).float()  # Adjust threshold to improve recall\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(pred_labels.cpu().numpy())\n",
        "\n",
        "plot_confusion_matrix(np.array(y_true), np.array(y_pred))"
      ],
      "metadata": {
        "id": "5_oYFN0RIvgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rxH376lQFZz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#treshold kam kr do"
      ],
      "metadata": {
        "id": "5QBq3tXEFbsG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}